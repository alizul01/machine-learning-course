{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alizul01/machine-learning-course/blob/main/praktikum_2_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmdd6QCpkQpc"
      },
      "source": [
        "## Library Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MTpdEIUnkFYA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "PlL2v56NaLZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28s6TBQaMfw",
        "outputId": "16e88c23-376a-4185-dd72-fcb0453a5842"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUQQ6SeFaVUv",
        "outputId": "548edf8d-fe3e-4b2d-aa6e-6452915fb46c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDKzf5EiaYoa",
        "outputId": "2238cf07-aef3-47a1-c801-fbf024c9e5d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3F8r_AVacnK",
        "outputId": "e952e1f8-a7d8-4671-b8a3-b87d901d82de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "SV-78mGgagTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARSMsaMjajQi",
        "outputId": "9ef17c2d-a02e-4dcb-8d24-2358b31635f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "oiokgtVNaonz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymxh3DRzasTX",
        "outputId": "f2eb6827-f29c-439d-c68f-e4297b2ab356"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "gVarNk_daxPs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJFhnFGmay_u",
        "outputId": "433655ef-a1b6-4366-b7a0-77adb1c1fef8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9evYRCrPbN3y",
        "outputId": "459e0d49-d7a8-4001-acd9-5a0870d2f403"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "nE9pt62dbnFU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ZK1JIXbuO1",
        "outputId": "4da4fe75-c70c-4b64-806d-982021d1a510"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "BYP3nw28bxzQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbDkjaHjb23V",
        "outputId": "b94fd51a-0385-4d51-de31-6b5ee6daf27e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "Y2zwv5Gvb6IU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qQbCz-Gb_su",
        "outputId": "f50c4f56-a464-4161-a999-dddc93b91955"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjmsBPOVcKzI",
        "outputId": "65bde774-2ad0-495e-bb55-64b5db1e6acb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "XJEXJBTLcPZy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ongAsXcTKh",
        "outputId": "d329c449-b726-4a2e-ed99-2fc40c4ce88b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "qkUWdgSDcVBs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzkrkw3LcX3b",
        "outputId": "91ef31d7-11a5-46bb-f11f-d70a26142e41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1KnKrjRcbOw",
        "outputId": "23b9610a-b46e-433b-9c08-88135b889ad9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "MnsS1rE5cekv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dihWKdibciXj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "j6O5645cck1_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxQFjaxEco91",
        "outputId": "f476d805-4418-4a82-e3d4-9e47be338229"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnd8VyD6csJz",
        "outputId": "27fbae23-ec96-40af-8766-caacb8c0398e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "StC0j8jNcte1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxezvEuWcw0L",
        "outputId": "3036088d-da0e-40e0-86ee-30fc1707febb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\" one attend him with a silver basin\\nFull of rose-water and bestrew'd with flowers,\\nAnother bear the \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"'He- Rqngbf3'jYCTkI- AFZgGrOM$mhNVVk- mXMxjlG&[UNK]?'jRmt[UNK]\\n3Vh&HZapnd TVF,qXnkVhH$Ajn,\\n!hJYNjfXr,$&hGSdX\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "GfPJsQx9c5G2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5qVrT7Mc6co",
        "outputId": "8d1c94ef-745a-4369-b423-6b1ec9a193a1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190919, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKW6l-Jpd1zq",
        "outputId": "29202c83-17f1-4282-db2a-906a07b0446b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.08349"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "oT57fA9Hd3b4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "avnUYUqBd4kk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "W5O3PNuHd8MI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITA4W9Bod-hB",
        "outputId": "c3674688-e519-48c4-9b7b-74703d5d9b1f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 17s 61ms/step - loss: 2.7234\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.9979\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.7188\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.5559\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.4567\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.3886\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3360\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2915\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2513\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2126\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1730\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1328\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0917\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0473\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0011\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9517\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9008\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.8490\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7968\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "Wijtl-SzeAuu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "7iRZFzgdfTvV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtrVjym5fVeH",
        "outputId": "b9f8c631-bae5-4824-e814-1ead9a4ced67"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "If sorelious Capulet which your tragh murder\n",
            "You stand fast; and then we buy with solemn reverent\n",
            "In my horse tears have done to the phocembly\n",
            "A gaggers arm our coverity: offer\n",
            "They're short, as thou, advise: mistake,\n",
            "As shall remain a pox our royal enter;\n",
            "And therein will speak as deep as neat.\n",
            "\n",
            "Provost:\n",
            "This is his thim!\n",
            "\n",
            "PROSPERO:\n",
            "Do so, then my awnable together\n",
            "Late o'erpower that he shall nevet ribam\n",
            "An every fortune fence am not ones, give\n",
            "me your horse serves, or valiant William wretched,\n",
            "And yet not hot;'t.\n",
            "\n",
            "Clown:\n",
            "Hath a husband till he will be the firmame.\n",
            "This will I met, we show me here bring me.\n",
            "And say I know thy words so well as one,\n",
            "Which without your mistress could to Wadower Juliet's daughter,\n",
            "Now thou couldst pile flainny-tongue--gain with\n",
            "shiftless brandings hy wife for an itle passage,\n",
            "Here lives not need to devose you:\n",
            "Deay heavy to myself and very weaphy.\n",
            "\n",
            "LUCIO:\n",
            "Even truth, for truth and till I left him.\n",
            "\n",
            "KING HENRY VI:\n",
            "Pardon her, my shapes that doth be teeked \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.113633871078491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAORDe4xfW8O",
        "outputId": "007c36a2-086a-4a6a-cdbd-41a7c6739588"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nOn which is thy seory woes, like a sea-cack?\\nFor forbid! Why, so I am asleep;\\nBut Front was time, offer but whilst thou,\\nIrents that thy beam to serve his friends.\\nBut he not son? behold, I prith! Implayation\\nIf not on the air, you know his wife, the other,\\nIf you were gone and tidings for my thoughts:\\nAimerle of heaven, and did upright\\nThe next gentleman to look on it; for therety\\nWar worn to lament that there, did Richard and\\nYet yet knew I here poor and rope thy way;\\nWe cannot the king mis-stating with him:\\nHis hathers are too rustle. Of him arm,\\nThy due by--pay we show 'em tamed,\\nOr that the inconstant doth begat\\nFormerly report in the hatches: I fear:\\nI am an executioner, his grave.\\n\\nEDWARD:\\nNo; I come return to live in pity\\nAnd through the vantage of the flowers: my fandship.\\nTreasons a thousand children but as they\\nTo thrive I want not partly turns.\\nHalf my inform, mislehs a duke to leave with thee!\\nThou shouldst thy dignity, that I had seen,\\nOr else your lasses faults, dispatc\"\n",
            " b\"ROMEO:\\nCome thee, as he fears defend that word,\\nThough they furlood after to tain's with fair,\\nErward my daughter's scolds: who at our most\\nHad wither'd had I not prophecy it!\\nNo doubt, my lord, God friend: get you the villain\\nWhich true prompts me well thou comest to-night!\\n\\nDUKE OF YORK:\\nThe news is true: may long I have.\\n\\nHENRY BOLINGBROKE:\\nGood did it up; so it must be a\\nCrettou love in banishment within.\\nIf you to act on, sight of fearture more\\nMore person from my brother's love,\\nNor any other griefs, making thee again:\\nI call me aught but hollow had.\\n\\nQUEEN MARGARET:\\nOff that once both our joy\\nThat away before this unwings with him?\\n\\nDUKE OF AUMERLE:\\nCan ere I would make words perpetual mad:\\nHow could he Ware his feel were there.\\n\\nKING HENRY VI:\\nBut I was banish'd: behold, as I hear,\\nYou please too light, to-nay my shore.\\n\\nPAULINA:\\nThere's the matter,--\\n\\nMERCUTIO:\\nI fear me not, Mowbray imagine,\\nThat habsh pursues\\nwith like elavion. As forsake his suret? hear how too doubh,\\nTo sound mo\"\n",
            " b\"ROMEO:\\nThe very seat of all present yeath?\\nIf I do so, then was done to her.\\n\\nPOLIXENES:\\nIf it be poor foo, I'll pay thee.\\n\\nCAPULET:\\nSo young and right, I hear, no.\\n\\nCORIOLANUS:\\nNo.\\n\\nThird Gentleman:\\nYeish way,\\nHis call humbly bring him disloyal,\\nThou hast not past the devil take again.\\n\\nAbHaRCENCE:\\nOr gentle in this needy haunt me from the like.\\nThus time have taketh Katharina will be dally\\nWith blisters that have bie'ded thee.\\n\\nCORIOLANUS:\\nI think\\nHe'll be hardy an in to fight in Padua,\\nTheir making traitors born. But he was never\\nShould be made to be made? Imanks, I pray you,\\nWarm thee! is he so serve a sleepy tribule, sir,\\nhath leisure shrive I never left him that\\ntherein which changed with old Claudio,\\nUnlips by grantering jeat-conference.\\n\\nANGELO:\\nDo not bid good my safely face to-day?\\nHow noble Warwick, Warwick!\\nCome hither, Eretermand and I'll lay\\nAnd for the thing I have.\\n\\nKATHARINA:\\nNo, but as cursed us, 'By course's bosom,\\nNever follow'd the fault'st. Unless what may friar so,\\nTak\"\n",
            " b\"ROMEO:\\nTwenty-taken: if the maid thou canst demand\\nThat ever you heard of such ainstrollign?\\n\\nQUEEN ELIZABETH:\\nMy cape?\\n\\nPOLIXENES:\\nA soldier, pursuing me no further.\\nPerhadact jongemoly in the field.\\n\\nKING HENRY VI:\\nAy, it shall be so. For I know the\\nway before I now, sir. Prithee, pray, Paris, Warwick!\\nWhy stay we?'twas now my suspicion? God forbid\\nIs like a bear to help you to the ground;\\nAnd he of worthiest weep the main apear.\\n\\nCLAUDIO:\\nCendern, thou shalt not be a Roman; and you have\\nwear it; for thee we banish with such life of a gods\\nOf what talk I ach, and first beging thee to\\nParel unavy slain; and how prepare for covern:\\nHad you seen the design, did meet him\\nVine. How now! what's the may be prock-well,\\nWhose honour but to flattering fearful boy,\\nHas deserved honour tears than art and date,\\nAnd for your gault. Green occupation shines\\nThe two brooker that three eyes therefore.\\n\\nEDWARD:\\nWhose lords, will you be so vouch a part\\nAn hagged back of love,\\nWhose rotten deny, the heavens to\"\n",
            " b\"ROMEO:\\n'Twas from your pleasure till you be with yourself:\\n'Tis time to enter of march to fight.\\n\\nMARIANA:\\nNo, Harry, here is come all throat. Thou liest!\\n\\nTRANIO:\\nI' faith, I'll need of you.\\n\\nHENRY BOLINGBROKE:\\nGo, rather, this bright shall be with thee.\\nThe feastous seest not heaven shake.\\nLether had thy business in their marriage\\nThat holds through the dispities of the world,\\nSound the father, and offended words.\\nMy wife comes Christian are less throne authority\\nMore than my daughter's jointure. Which\\n'fore the prince's return, and it royal day!\\n\\nBIANCA:\\nBear him to your house: but I cannot help,\\nYet give some life. I do so.\\nI am the grace of your crowns body;\\nThy breathers climis, his name is need, together:\\nEither thought you hide your remedy person;\\nOt something that thou shriek, with all despair:\\nIn brief, starks; only for a dish'd die.\\nBlow'd the oath five and people's veinsure. Be much.\\n\\nQUEEN ELIZABETH:\\nFarewell! Why, here that God hath he rotes?\\nAlas, I beseech you, make me so, fr\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.514010906219482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_XMudcfaLq",
        "outputId": "9e848e2d-6e30-4288-c822-2e44c742537e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7bebe0f856c0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujlLv3rIfb_5",
        "outputId": "18595214-6a89-426e-ef63-70d2cf779ba6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The boy is temper'd: she is so made again,\n",
            "That he would buzzard the life-woman contented.\n",
            "The trib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "eObNIcTyfeDb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "  vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "oJy8ZZ7yfpkI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "rZl5aiQ6fsNb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhZN25lqfvYm",
        "outputId": "c19987ff-c951-4ef9-f9c4-36fceaf8adf9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 66ms/step - loss: 2.7235\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7beb6178e5c0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Owo127kfxBK",
        "outputId": "8816c1d3-6a51-4f13-f93c-bb6748d76e0f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1729\n",
            "Epoch 1 Batch 50 Loss 2.0240\n",
            "Epoch 1 Batch 100 Loss 1.8954\n",
            "Epoch 1 Batch 150 Loss 1.8681\n",
            "\n",
            "Epoch 1 Loss: 1.9960\n",
            "Time taken for 1 epoch 13.61 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7960\n",
            "Epoch 2 Batch 50 Loss 1.7572\n",
            "Epoch 2 Batch 100 Loss 1.6633\n",
            "Epoch 2 Batch 150 Loss 1.6514\n",
            "\n",
            "Epoch 2 Loss: 1.7149\n",
            "Time taken for 1 epoch 11.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6080\n",
            "Epoch 3 Batch 50 Loss 1.5777\n",
            "Epoch 3 Batch 100 Loss 1.6170\n",
            "Epoch 3 Batch 150 Loss 1.5151\n",
            "\n",
            "Epoch 3 Loss: 1.5511\n",
            "Time taken for 1 epoch 11.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4712\n",
            "Epoch 4 Batch 50 Loss 1.4641\n",
            "Epoch 4 Batch 100 Loss 1.4092\n",
            "Epoch 4 Batch 150 Loss 1.4350\n",
            "\n",
            "Epoch 4 Loss: 1.4506\n",
            "Time taken for 1 epoch 11.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4045\n",
            "Epoch 5 Batch 50 Loss 1.4002\n",
            "Epoch 5 Batch 100 Loss 1.3767\n",
            "Epoch 5 Batch 150 Loss 1.3607\n",
            "\n",
            "Epoch 5 Loss: 1.3821\n",
            "Time taken for 1 epoch 20.59 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3673\n",
            "Epoch 6 Batch 50 Loss 1.3827\n",
            "Epoch 6 Batch 100 Loss 1.3372\n",
            "Epoch 6 Batch 150 Loss 1.3135\n",
            "\n",
            "Epoch 6 Loss: 1.3295\n",
            "Time taken for 1 epoch 11.54 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2947\n",
            "Epoch 7 Batch 50 Loss 1.2808\n",
            "Epoch 7 Batch 100 Loss 1.3090\n",
            "Epoch 7 Batch 150 Loss 1.3140\n",
            "\n",
            "Epoch 7 Loss: 1.2848\n",
            "Time taken for 1 epoch 10.95 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2158\n",
            "Epoch 8 Batch 50 Loss 1.2508\n",
            "Epoch 8 Batch 100 Loss 1.2595\n",
            "Epoch 8 Batch 150 Loss 1.2381\n",
            "\n",
            "Epoch 8 Loss: 1.2437\n",
            "Time taken for 1 epoch 11.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1609\n",
            "Epoch 9 Batch 50 Loss 1.1453\n",
            "Epoch 9 Batch 100 Loss 1.2294\n",
            "Epoch 9 Batch 150 Loss 1.1815\n",
            "\n",
            "Epoch 9 Loss: 1.2041\n",
            "Time taken for 1 epoch 11.08 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1349\n",
            "Epoch 10 Batch 50 Loss 1.1397\n",
            "Epoch 10 Batch 100 Loss 1.1767\n",
            "Epoch 10 Batch 150 Loss 1.1673\n",
            "\n",
            "Epoch 10 Loss: 1.1639\n",
            "Time taken for 1 epoch 11.20 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentu saja, mari kita bedakan antara dua kelas yang ditentukan dalam segmen kode yang disediakan.\n",
        "\n",
        "1. **CustomTraining Class:**\n",
        "\n",
        "   - Tujuan: Kelas ini digunakan untuk melatih model bahasa.\n",
        "   - Mewarisi dari: Tampaknya mewarisi dari kelas bernama `Model Saya`, yang tidak ditentukan dalam kode yang disediakan.\n",
        "   - Metode Utama:\n",
        "     - `train_step(self, inputs)`: Metode ini menentukan langkah pelatihan khusus untuk model. Ini menghitung gradien, menerapkannya, dan memperbarui bobot model.\n",
        "   - Penggunaan: Digunakan untuk melatih model menggunakan loop pelatihan khusus.\n",
        "\n",
        "2. **OneStep Class:**\n",
        "\n",
        "   - Tujuan: Kelas ini digunakan untuk menghasilkan teks menggunakan model bahasa terlatih.\n",
        "   - Mewarisi dari: Tidak mewarisi kelas tertentu, tetapi tampaknya merupakan subkelas dari `tf.keras.Model`.\n",
        "   - Metode Utama:\n",
        "     - `generate_one_step(self, inputs, States=None)`: Metode ini menghasilkan teks selangkah demi selangkah. Dibutuhkan urutan masukan dan menghasilkan karakter berikutnya berdasarkan prediksi model yang dilatih.\n",
        "   - Penggunaan: Digunakan untuk menghasilkan teks dengan memberinya urutan awal dan memungkinkannya menghasilkan karakter berikutnya selangkah demi selangkah.\n",
        "\n",
        "Kedua kelas ini memiliki tujuan berbeda dalam konteks bekerja dengan model bahasa. Kelas `CustomTraining` bertanggung jawab untuk melatih model, sedangkan kelas `OneStep` digunakan untuk menghasilkan teks menggunakan model yang dilatih."
      ],
      "metadata": {
        "id": "BbobJrBqiLLI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNisRexI2RH1W4dy6HH4EzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}